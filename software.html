<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--
Design by TEMPLATED
http://templated.co
Released for free under the Creative Commons Attribution License

Name       : Skeleton 
Description: A two-column, fixed-width design with dark color scheme.
Version    : 1.0
Released   : 20130902

-->
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Lucas Rencker's webpage</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900" rel="stylesheet" />
<link href="default.css" rel="stylesheet" type="text/css" media="all" />
<link href="fonts.css" rel="stylesheet" type="text/css" media="all" />
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

</head>
<body>
<div id="page" class="container">
	<div id="header">
		<div id="logo">
			<img src="images/IMG_6631.jpg" alt=""  width="150">
			<h1><a href="#">Lucas Rencker</a></h1>
		</div>
		<div id="menu">
			<ul>
				<li><a href="index.html" accesskey="1" title="">Homepage</a></li>
				<li><a href="publications.html" accesskey="2" title="">Publications</a></li>
				<li class="current_page_item"><a href="software.html" accesskey="3" title="">Software/Demos</a></li>
				<li><a href="cv.html" accesskey="4" title="">CV</a></li>
				<li><a href="contact.html" accesskey="5" title="">Contact</a></li>
			</ul>
		</div>
	</div>
	<div id="main">
		<div class="title">
		<h1> Software/Demos </h1>
		</div>
			<p> You can find some demos and software here: 
			
			<ul style="list-style-type:disc">
			<li><a href="#DL_for_declipping">Consistent dictionary learning for signal declipping</a></li>
			<li><a href="#greedy_algorithm">A greedy algorithm with learned statistics for sparse signal reconstruction</a></li>
			</ul>
			</p>
			
			<a name="DL_for_declipping"></a> 

			<h2> Consistent dictionary learning for signal declipping </h2>
			
			<hr>
			
			<p> Clipping is a common nonlinear distortion in signal processing. 
			Clipping often happens due to dynamic range limitations in software or hardware systems, when the waveform is truncated above a certain level:</p>
			
			<p style="text-align:center;"> <img src="images/clipped_glockenspiel.png" alt="" width="600" align="middle"> <p/>
						
  \(
\def\x{{\mathbf x}}
\def\z{{\mathbf z}}
\def\y{\mathbf{y}}

\def\D{{\mathbf D}}

\DeclareMathOperator{\Mr}{\mathbf{M^r}}
\DeclareMathOperator{\Mcp}{\mathbf{M^{c+}}}
\DeclareMathOperator{\Mcm}{\mathbf{M^{c-}}}
\DeclareMathOperator{\bfalpha}{\alpha}
\DeclareMathOperator{\Cyt}{\mathcal{C}(\y_t)}
\DeclareMathOperator{\Cy}{\mathcal{C}(\y)}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\dEU}{d}
\)
  <p>
  The received signal can be written as:
  
\[
\y = \Mr \x + \theta^+ \Mcp \mathbf{1} + \theta^- \Mcm \mathbf{1},
\]
where \(\x\) is the input signal, \(\mathbf{1}\) is the all-ones vector in \(\mathbb{R}^{N}\), \(\theta^+/\theta^-\) are positive negative clipping thesholds respectively, and \(\Mr, \Mcp\) and \(\Mcm\) are diagonal binary sensing matrices that define the reliable, positive and negative clipped samples respectively.
</p>
<p> Recovering a clipped signal is a challenging inverse problem, since the distortion is nonlinear and high energy samples are missing. </p>

<p> Declipping can be treated as a signal inpainting problem, i.e. discarding the clipped samples and performing sparse decomposition on the reliable (unclipped) samples:</p>
\[
\min_{\bfalpha \in \mathbb{R}^M} \|\Mr(\y-\D\bfalpha)\|_2^2 \quad \text{s.t.} \quad \|\bfalpha\|_0\leq K.
\]
<p>However this approach fails when the signal is highly clipped. Here is an example of declipping a glockenspiel signal (here 73% of the samples are clipped), using Iterative Hard Thresholding (IHT) with a twice redundant DCT dictionary:</p>

<p style="text-align:center;"> <img src="images/declip_glockenspiel_IHT.png" alt="" width="600" align="middle"> <p/>

<p> The algorithms fails to reconstruct the clipped samples from the low energy unclipped samples. Dictionary learning has been shown to lead to reconstruction improvement in various inverse problems such as denoising or inpainting, compared to a fixed DCT dictionary. 
Dictionary learning for signal declipping can also be formulated by simply discarding the clipped samples and learning from the reliable samples:</p>
\[
\min_{\D \in \mathcal{D}, \alpha_t}  \sum_t\|\Mr_t(\y_t-\D\bfalpha_t)\|_2^2 \quad \text{s.t.} \quad \forall t,  \|\bfalpha_t\|_0\leq K
\]
<p>However, state-of-the-art dictionary learning algorithms also fail to reconstruct clipped signals, as can be seen here:</p>
<p style="text-align:center;"> <img src="images/declip_glockenspiel_DL.png" alt="" width="600" align="middle"> <p/>

<p>We proposed a dictionary learning algorithm that enforces <i>consistency</i> with the clipped samples, i.e. it enforces the reconstructed clipped samples to be above the clipping threshold. 
This can be formulated as:
\[
\min_{\D \in \mathcal{D}, \bfalpha_t} \sum_t \dEU (\D\bfalpha_t, \mathcal{C}(\y_t))^2 \quad \text{s.t.} \quad \forall t,  \|\bfalpha_t\|_0\leq K,
\]
where \(\mathcal{C}(\y)\) is the set of admissible signals, i.e. the set of signals that are consistent with the measured signal \(\y\):
\[
\mathcal{C}(\y) \triangleq \{\x| \Mr \y=\Mr \x, \Mcp \x \succeq \Mcp \y, \Mcm \x \preceq \Mcm \y\},
\]
and \(\dEU (\x, \mathcal{C}(\y))\) is the Euclidean distance between the estimated signal \(\x\) and the set \(\mathcal{C}(\y)\):
\[
\dEU (\x, \mathcal{C}(\y)) = \min_{\z \in \mathcal{C}(\y)} \|\x-\z\|_2.
\]
</p>
<p>The proposed algorithm leads to significant performance improvement compared to state-of-the art dictionary learning algorithms applied to declipping:</p>
<p style="text-align:center;"> <img src="images/declip_glockenspiel_consDL.png" alt="" width="600" align="middle"> <p/>

<h2> Audio examples:</h2>

<p>
We here compare 4 different approaches:
 <ul style="list-style-type:disc">
  <li> <b>IHT:</b> discards the clipped samples and performs sparse coding using Iterative Hard Thresholding (IHT) and a fixed DCT dictionary</li>
  <li> <b>Dictionary Learning:</b> discards the clipped samples and performs a simple gradient descent-based dictionary learning algorithm (learnt on the unclipped samples)</li>
  <li> <b>Consistent IHT:</b> performs consistent sparse coding (i.e. enforces the clipped samples to be above the clipping threshold), using a fixed DCT dictionary [1]</li>
  <li> <b>Consistent dictionary learning:</b> performs consistent dictionary learning, as proposed in [2]</li>
</ul>
</p>
<p>
References:
</p>
<p>
[1]: <b>Consistent iterative hard thresholding for signal declipping</b>, <br>
   S. Kitic, L. Jacques, N. Madhu, M. P. Hopwood, A. Spriet, C. De Vleeschouwer, ICASSP, 2013<br>
</p>
<p>
[2]: <b>Consistent dictionary learning for signal declipping</b>, <br>
    L. Rencker, F. Bach, W. Wang, M. D. Plumbley, Latent Variable Analysis and Signal Separation (LVA/ICA), Guildford, UK, 2018
</p>

<p>
You can find the paper <a href="http://epubs.surrey.ac.uk/846156/1/Consistent_DL_for_signal_declipping.pdf">here</a> and the MATLAB and Python code on my <a href="https://github.com/LucasRr/">github page</a>.
</p>

<table>
  <tr>
    <th rowspan="6">Music 1:</th>
    <th>Clean: </th>
    <td><audio controls> <source src="files/audio/music01_clean.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Clipped (5 dB): </th>
    <td> <audio controls> <source src="files/audio/music01_clipped_5dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>IHT (0.75 dB): </th>
    <td> <audio controls> <source src="files/audio/music01_declipped_IHT_0_75dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Dictionary learning (0.79 dB): </th>
    <td><audio controls> <source src="files/audio/music01_declipped_DL_0_79dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Consistent IHT (6.41 dB): </th>
    <td><audio controls> <source src="files/audio/music01_declipped_consIHT_6_41dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Consistent dictionary learning (7.68 dB): </th>
    <td><audio controls> <source src="files/audio/music01_declipped_consDL_7_68dB.wav" type="audio/wav"> </audio></td>
  </tr>
   
  <tr>
    <th rowspan="6">Music 2:</th>
    <th>Clean: </th>
    <td><audio controls> <source src="files/audio/music02_clean.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Clipped (5 dB): </th>
    <td> <audio controls> <source src="files/audio/music02_clipped_5dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>IHT (0.70 dB): </th>
    <td> <audio controls> <source src="files/audio/music02_declipped_IHT_0_70dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Dictionary learning (0.75 dB): </th>
    <td><audio controls> <source src="files/audio/music02_declipped_DL_0_75dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Consistent IHT (8.24 dB): </th>
    <td><audio controls> <source src="files/audio/music02_declipped_consIHT_8_24dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Consistent dictionary learning (10.30 dB): </th>
    <td><audio controls> <source src="files/audio/music02_declipped_consDL_10_30dB.wav" type="audio/wav"> </audio></td>
  </tr>
  
<tr>
    <th rowspan="6">Female speech:</th>
    <th>Clean: </th>
    <td><audio controls> <source src="files/audio/female_clean.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Clipped (5 dB): </th>
    <td> <audio controls> <source src="files/audio/female_clipped_5dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>IHT (0.83 dB): </th>
    <td> <audio controls> <source src="files/audio/female_declipped_IHT_0_83dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Dictionary learning (0.91 dB): </th>
    <td><audio controls> <source src="files/audio/female_declipped_DL_0_91dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Consistent IHT (6.30 dB): </th>
    <td><audio controls> <source src="files/audio/female_declipped_consIHT_6_30dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Consistent dictionary learning (7.04 dB): </th>
    <td><audio controls> <source src="files/audio/female_declipped_consDL_7_04dB.wav" type="audio/wav"> </audio></td>
  </tr>

  <tr>
    <th rowspan="6">Male speech:</th>
    <th>Clean: </th>
    <td><audio controls> <source src="files/audio/male_clean.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Clipped (5 dB): </th>
    <td> <audio controls> <source src="files/audio/male_clipped_5dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>IHT (0.99 dB): </th>
    <td> <audio controls> <source src="files/audio/male_declipped_IHT_0_99dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Dictionary learning (1.06 dB): </th>
    <td><audio controls> <source src="files/audio/male_declipped_DL_1_06dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Consistent IHT (6.71 dB): </th>
    <td><audio controls> <source src="files/audio/male_declipped_consIHT_6_71dB.wav" type="audio/wav"> </audio></td>
  </tr>
  <tr>
    <th>Consistent dictionary learning (8.12 dB): </th>
    <td><audio controls> <source src="files/audio/male_declipped_consDL_8_12dB.wav" type="audio/wav"> </audio></td>
</tr>
  
</table>

<br><br>

<a name="greedy_algorithm"></a> 
<h2> A greedy algorithm with learned statistics for sparse signal reconstruction </h2>

<hr>

<p> Orthogonal Matching Pursuit (OMP) is a greedy algorithm that solves:
\[
\min_{\bfalpha \in \mathbb{R}^M} \|\y-\D\bfalpha\|_2^2 \quad \text{s.t.} \quad \|\bfalpha\|_0\leq K.
\]
using coordinate descent, i.e. optimizing each coefficient one at a time. This can be seen as a Maximum-Likelihood (ML) estimate of the coefficients under a Gaussian noise, along with a sparsity constraint.
</p>
<p>
We proposed instead to solve a Maximum-A-Posteriori estimation, under a Gaussian prior:
\[
\min_{\bfalpha \in \mathbb{R}^M} (\y-\D\bfalpha)^T\Sigma^{-1}(\y-\D\bfalpha) + (\bfalpha-\mu)^T\Lambda^{-1}(\bfalpha-\mu) \quad \text{s.t.} \quad \|\bfalpha\|_0\leq K,
\]
where \(\mu\) and \(\Lambda\) are the mean and covariance of the sparse coefficients respectively, and \(\Sigma\) is the noise covariance matrix. The mean and covariance are simple first order statistics, and can be learned on from a clean training image.
The mean and covariance provide extra information that can help the reconstruction. In particular, the diagonal elements of the covariance matrix model the a-priori energy of the sparse coefficients, and the non-diagonal elements model the correlation between coefficients.

We proposed a greedy OMP-like algorithm that minimizes the proposed MAP formulation. We compare with the baseline OMP, and the recently proposed Covariance-Assisted Matching Pursuit (CAMP). The proposed algorithms shows significant improvement image reconstruction from a few noisy measurements. 

<p style="text-align:center;"> <img src="images/greedy_algorithm_learned_statistics.png" alt="" width="600" align="middle"> <p/>

The paper can be found <a href="http://epubs.surrey.ac.uk/813519/1/Greedy_algorithm_with_learned_statistics.pdf">here</a>. 

	</div>
</div>
</body>
</html>
